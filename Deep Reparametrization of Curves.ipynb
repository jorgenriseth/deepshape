{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet\n",
    "using Plots\n",
    "using Reparam\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module dev.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Main.dev"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module dev\n",
    "using LinearAlgebra: norm\n",
    "using Knet\n",
    "\n",
    "function squaresum(x)\n",
    "    out = x[1]^2\n",
    "    for i in 2:length(x)\n",
    "        out += x[i]^2\n",
    "    end\n",
    "    out\n",
    "end\n",
    "\n",
    "\n",
    "struct Qmap{T<:Function, TF<:Real}; c::T; h::TF; end\n",
    "(q::Qmap)(x) = sqrt(norm(0.5*(q.c(x+q.h) - q.c(x-q.h)) / q.h)) * q.c(x)\n",
    "\n",
    "\n",
    "struct SineLayer{T<:AbstractVector}; N::Int; c::Param{T}; end \n",
    "SineLayer(c::Vector{T})  where {T<:Real} = SineLayer(length(c), Param(c))\n",
    "\n",
    "function (S::SineLayer)(x, y)\n",
    "    z = sin(π*x) * S.c[1]\n",
    "    y1 = π * cos(π*x) * S.c[1]\n",
    "    for n in 2:S.N\n",
    "        z += sin(n*π*x) * S.c[n]\n",
    "        y1 += n * π * cos(n*π*x) * S.c[n]\n",
    "    end\n",
    "    return x + z, (1. + y1 )* y\n",
    "end\n",
    "\n",
    "(S::SineLayer)(x::Real) = S(x, 1.)\n",
    "(S::SineLayer)(args) = S(args...)\n",
    "\n",
    "\n",
    "Loss(q::Vector{T}, r::Vector{T}) where {T<:Real} = squaresum(q-r)\n",
    "Loss(Q::Vector{T}, R::Vector{T}) where {T<:AbstractVector} = sum(Loss.(Q, R))\n",
    "\n",
    "end # module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Param{Array{Int64,1}}:\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Param([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "TypeError: in Type{...} expression, expected UnionAll, got a value of type typeof(supertype)",
     "output_type": "error",
     "traceback": [
      "TypeError: in Type{...} expression, expected UnionAll, got a value of type typeof(supertype)",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[336]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091",
      " [3] execute_code(::String, ::String) at /home/jorgen/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:27",
      " [4] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /home/jorgen/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:86",
      " [5] #invokelatest#1 at ./essentials.jl:710 [inlined]",
      " [6] invokelatest at ./essentials.jl:709 [inlined]",
      " [7] eventloop(::ZMQ.Socket) at /home/jorgen/.julia/packages/IJulia/rWZ9e/src/eventloop.jl:8",
      " [8] (::IJulia.var\"#15#18\")() at ./task.jl:356"
     ]
    }
   ],
   "source": [
    "supertype{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Usage:\n",
       "\n",
       "\\begin{verbatim}\n",
       "x = Param([1,2,3])          # user declares parameters with `Param`\n",
       "x => P([1,2,3])             # `Param` is just a struct wrapping a value\n",
       "value(x) => [1,2,3]         # `value` returns the thing wrapped\n",
       "sum(x .* x) => 14           # Params act like regular values\n",
       "y = @diff sum(x .* x)       # Except when we differentiate using `@diff`\n",
       "y => T(14)                  # you get another struct\n",
       "value(y) => 14              # which carries the same result\n",
       "params(y) => [x]            # and the Params that it depends on \n",
       "grad(y,x) => [2,4,6]        # and the gradients for all Params\n",
       "\\end{verbatim}\n",
       "\\texttt{Param(x)} returns a struct that acts like \\texttt{x} but marks it as a parameter you want to compute gradients with respect to.\n",
       "\n",
       "\\texttt{@diff expr} evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.\n",
       "\n",
       "\\texttt{grad(y, x)} returns the gradient of \\texttt{y} (output by @diff) with respect to any parameter \\texttt{x::Param}, or  \\texttt{nothing} if the gradient is 0.\n",
       "\n",
       "\\texttt{value(x)} returns the value associated with \\texttt{x} if \\texttt{x} is a \\texttt{Param} or the output of \\texttt{@diff}, otherwise returns \\texttt{x}.\n",
       "\n",
       "\\texttt{params(x)} returns an iterator of Params found by a recursive search of object \\texttt{x}.\n",
       "\n",
       "Alternative usage:\n",
       "\n",
       "\\begin{verbatim}\n",
       "x = [1 2 3]\n",
       "f(x) = sum(x .* x)\n",
       "f(x) => 14\n",
       "grad(f)(x) => [2 4 6]\n",
       "gradloss(f)(x) => ([2 4 6], 14)\n",
       "\\end{verbatim}\n",
       "Given a scalar valued function \\texttt{f}, \\texttt{grad(f,argnum=1)} returns another function \\texttt{g} which takes the same inputs as \\texttt{f} and returns the gradient of the output with respect to the argnum'th argument. \\texttt{gradloss} is similar except the resulting function also returns f's output.\n",
       "\n"
      ],
      "text/markdown": [
       "Usage:\n",
       "\n",
       "```\n",
       "x = Param([1,2,3])          # user declares parameters with `Param`\n",
       "x => P([1,2,3])             # `Param` is just a struct wrapping a value\n",
       "value(x) => [1,2,3]         # `value` returns the thing wrapped\n",
       "sum(x .* x) => 14           # Params act like regular values\n",
       "y = @diff sum(x .* x)       # Except when we differentiate using `@diff`\n",
       "y => T(14)                  # you get another struct\n",
       "value(y) => 14              # which carries the same result\n",
       "params(y) => [x]            # and the Params that it depends on \n",
       "grad(y,x) => [2,4,6]        # and the gradients for all Params\n",
       "```\n",
       "\n",
       "`Param(x)` returns a struct that acts like `x` but marks it as a parameter you want to compute gradients with respect to.\n",
       "\n",
       "`@diff expr` evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.\n",
       "\n",
       "`grad(y, x)` returns the gradient of `y` (output by @diff) with respect to any parameter `x::Param`, or  `nothing` if the gradient is 0.\n",
       "\n",
       "`value(x)` returns the value associated with `x` if `x` is a `Param` or the output of `@diff`, otherwise returns `x`.\n",
       "\n",
       "`params(x)` returns an iterator of Params found by a recursive search of object `x`.\n",
       "\n",
       "Alternative usage:\n",
       "\n",
       "```\n",
       "x = [1 2 3]\n",
       "f(x) = sum(x .* x)\n",
       "f(x) => 14\n",
       "grad(f)(x) => [2 4 6]\n",
       "gradloss(f)(x) => ([2 4 6], 14)\n",
       "```\n",
       "\n",
       "Given a scalar valued function `f`, `grad(f,argnum=1)` returns another function `g` which takes the same inputs as `f` and returns the gradient of the output with respect to the argnum'th argument. `gradloss` is similar except the resulting function also returns f's output.\n"
      ],
      "text/plain": [
       "  Usage:\n",
       "\n",
       "\u001b[36m  x = Param([1,2,3])          # user declares parameters with `Param`\u001b[39m\n",
       "\u001b[36m  x => P([1,2,3])             # `Param` is just a struct wrapping a value\u001b[39m\n",
       "\u001b[36m  value(x) => [1,2,3]         # `value` returns the thing wrapped\u001b[39m\n",
       "\u001b[36m  sum(x .* x) => 14           # Params act like regular values\u001b[39m\n",
       "\u001b[36m  y = @diff sum(x .* x)       # Except when we differentiate using `@diff`\u001b[39m\n",
       "\u001b[36m  y => T(14)                  # you get another struct\u001b[39m\n",
       "\u001b[36m  value(y) => 14              # which carries the same result\u001b[39m\n",
       "\u001b[36m  params(y) => [x]            # and the Params that it depends on \u001b[39m\n",
       "\u001b[36m  grad(y,x) => [2,4,6]        # and the gradients for all Params\u001b[39m\n",
       "\n",
       "  \u001b[36mParam(x)\u001b[39m returns a struct that acts like \u001b[36mx\u001b[39m but marks it as a parameter you\n",
       "  want to compute gradients with respect to.\n",
       "\n",
       "  \u001b[36m@diff expr\u001b[39m evaluates an expression and returns a struct that contains the\n",
       "  result (which should be a scalar) and gradient information.\n",
       "\n",
       "  \u001b[36mgrad(y, x)\u001b[39m returns the gradient of \u001b[36my\u001b[39m (output by @diff) with respect to any\n",
       "  parameter \u001b[36mx::Param\u001b[39m, or \u001b[36mnothing\u001b[39m if the gradient is 0.\n",
       "\n",
       "  \u001b[36mvalue(x)\u001b[39m returns the value associated with \u001b[36mx\u001b[39m if \u001b[36mx\u001b[39m is a \u001b[36mParam\u001b[39m or the output\n",
       "  of \u001b[36m@diff\u001b[39m, otherwise returns \u001b[36mx\u001b[39m.\n",
       "\n",
       "  \u001b[36mparams(x)\u001b[39m returns an iterator of Params found by a recursive search of\n",
       "  object \u001b[36mx\u001b[39m.\n",
       "\n",
       "  Alternative usage:\n",
       "\n",
       "\u001b[36m  x = [1 2 3]\u001b[39m\n",
       "\u001b[36m  f(x) = sum(x .* x)\u001b[39m\n",
       "\u001b[36m  f(x) => 14\u001b[39m\n",
       "\u001b[36m  grad(f)(x) => [2 4 6]\u001b[39m\n",
       "\u001b[36m  gradloss(f)(x) => ([2 4 6], 14)\u001b[39m\n",
       "\n",
       "  Given a scalar valued function \u001b[36mf\u001b[39m, \u001b[36mgrad(f,argnum=1)\u001b[39m returns another function\n",
       "  \u001b[36mg\u001b[39m which takes the same inputs as \u001b[36mf\u001b[39m and returns the gradient of the output\n",
       "  with respect to the argnum'th argument. \u001b[36mgradloss\u001b[39m is similar except the\n",
       "  resulting function also returns f's output."
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#62 (generic function with 1 method)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(t) = [cos(2π*t), sin(2π*t)]\n",
    "γ(t) = 0.9t^2 + 0.1t \n",
    "\n",
    "c2 = c ∘ γ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Main.dev.Qmap{Base.var\"#62#63\"{typeof(c),typeof(γ)},Float64}(Base.var\"#62#63\"{typeof(c),typeof(γ)}(c, γ), 0.0001)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = dev.Qmap(c, 1e-4)\n",
    "r = dev.Qmap(c2, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0:0.0009775171065493646:1.0"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = range(0, 1, length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Main.dev.SineLayer{Array{Float64,1}}(2, P(Array{Float64,1}(2)))"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = dev.SineLayer(zeros(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6465.8123731737805"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myparams = @diff S.(X)\n",
    "\n",
    "out = S.(X)\n",
    "\n",
    "Q = q.(X)\n",
    "R = [sqrt(y) * r(z) for (z, y) in out]\n",
    "\n",
    "loss = @diff dev.Loss(Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 190.211243502146\n",
       " 233.40672876467266"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = @diff sum(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::AutoGrad.var\"#gradfun#7\"{AutoGrad.var\"#gradfun#6#8\"{typeof(Main.dev.Loss),Int64,Bool}}) (generic function with 1 method)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradloss = grad(dev.Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching grad(::Array{Float64,1})\nClosest candidates are:\n  grad(::Any, !Matched::Any) at /home/jorgen/.julia/packages/AutoGrad/VFrAv/src/core.jl:215\n  grad(!Matched::AutoGrad.Tape, !Matched::AutoGrad.Tracked) at /home/jorgen/.julia/packages/AutoGrad/VFrAv/src/core.jl:216\n  grad(!Matched::Function) at /home/jorgen/.julia/packages/AutoGrad/VFrAv/src/core.jl:219\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching grad(::Array{Float64,1})\nClosest candidates are:\n  grad(::Any, !Matched::Any) at /home/jorgen/.julia/packages/AutoGrad/VFrAv/src/core.jl:215\n  grad(!Matched::AutoGrad.Tape, !Matched::AutoGrad.Tracked) at /home/jorgen/.julia/packages/AutoGrad/VFrAv/src/core.jl:216\n  grad(!Matched::Function) at /home/jorgen/.julia/packages/AutoGrad/VFrAv/src/core.jl:219\n  ...",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[378]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091",
      " [3] execute_code(::String, ::String) at /home/jorgen/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:27",
      " [4] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /home/jorgen/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:86",
      " [5] #invokelatest#1 at ./essentials.jl:710 [inlined]",
      " [6] invokelatest at ./essentials.jl:709 [inlined]",
      " [7] eventloop(::ZMQ.Socket) at /home/jorgen/.julia/packages/IJulia/rWZ9e/src/eventloop.jl:8",
      " [8] (::IJulia.var\"#15#18\")() at ./task.jl:356"
     ]
    }
   ],
   "source": [
    "grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param[]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(loss)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
